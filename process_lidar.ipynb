{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from typing import Any\n",
    "from multiprocessing import Pool\n",
    "from statistics import mode as stats_mode\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import dill\n",
    "import laspy\n",
    "import CSF\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import open3d as o3d\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.stats import pointbiserialr, mode as scipy_mode\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import analysis_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSES = 8\n",
    "\n",
    "LIDAR_CLASSIFICATION_CODES = {\n",
    "    0: 'never classified',\n",
    "    1: 'unassigned',\n",
    "    2: 'ground',\n",
    "    3: 'low vegetation',\n",
    "    4: 'medium vegetation',\n",
    "    5: 'high vegetation',\n",
    "    6: 'building',\n",
    "    7: 'noise',\n",
    "    8: 'model key/reserved',\n",
    "    9: 'water',\n",
    "    10: 'rail',\n",
    "    11: 'road surface',\n",
    "    12: 'overlap/reserved',\n",
    "    13: 'wire - guard',\n",
    "    14: 'wire - conductor',\n",
    "    15: 'transmission tower',\n",
    "    16: 'wire - connector',\n",
    "    17: 'bridge deck',\n",
    "    18: 'high noise',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: https://opendata.vancouver.ca/explore/dataset/lidar-2018/information/\n",
    "\n",
    "# # Kits\n",
    "# data_folder = Path('./data/4870E_54560N_kits')\n",
    "# las_file_path = data_folder / '4870E_54560N.las'\n",
    "\n",
    "# Point Grey forest + buildings\n",
    "data_folder = Path('./data/483000_5457000_ptgrey')\n",
    "las_file_path = data_folder / '483000_5457000.las'\n",
    "\n",
    "# # Downtown buildings\n",
    "# data_folder = Path('./data/491000_5458000_downtown')\n",
    "# las_file_path = data_folder / '491000_5458000.las'\n",
    "\n",
    "las_file = laspy.read(las_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class to make the las data easier to manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LasPointData:\n",
    "    def __init__(self, las: laspy.LasData) -> None:\n",
    "        self.las = las\n",
    "        self.header = las.header\n",
    "        self.vlrs = las.vlrs\n",
    "        self.point_format = las.point_format\n",
    "        self.point_count = las.header.point_count\n",
    "\n",
    "        self.point_dimensions = ['x', 'y', 'z'] + [d.name for d in las.point_format.dimensions]\n",
    "\n",
    "        self.data = {}\n",
    "        for dim in self.point_dimensions:\n",
    "            self.data[dim] = np.asarray(getattr(las.points, dim))\n",
    "        \n",
    "    def header_info(self) -> None:\n",
    "        for attr in dir(self.header):\n",
    "            if attr.startswith('_') or attr.isupper():\n",
    "                continue\n",
    "            if isinstance(val := getattr(self.header, attr), MethodType):\n",
    "                continue\n",
    "            print(f'{attr}: {val}')\n",
    "    \n",
    "    def crs(self) -> None:\n",
    "        for vlr in self.vlrs:\n",
    "            try:\n",
    "                print(repr(vlr.parse_crs()))\n",
    "            except AttributeError:\n",
    "                pass\n",
    "    \n",
    "    def get_point_format(self) -> tuple[laspy.PointFormat, list[str]]:\n",
    "        return self.point_format, self.point_dimensions\n",
    "    \n",
    "    def get_point_data(self, scaled: bool = True, step: int = 1, normalized: bool = False) -> np.ndarray:\n",
    "        if normalized and not scaled:\n",
    "            raise ValueError('Unscaled data cannot be normalized')\n",
    "        if normalized and not hasattr(self, 'z_normalized'):\n",
    "            raise AttributeError(f'Attribute \\'z_normalized\\' does not exist')\n",
    "\n",
    "        x = self.x if scaled else self.X\n",
    "        y = self.y if scaled else self.Y\n",
    "        z = self.data['z_normalized' if normalized else 'z'] if scaled else self.Z\n",
    "        data = np.vstack((x, y, z)).T\n",
    "\n",
    "        if step > 1:\n",
    "            mask = np.zeros(self.point_count, dtype=bool)\n",
    "            mask[::step] = True\n",
    "            return data[mask]\n",
    "        return data\n",
    "    \n",
    "    def get_class_names(self) -> np.ndarray:\n",
    "        lcc_get_partial = lambda code: LIDAR_CLASSIFICATION_CODES.get(code, '')\n",
    "        return np.vectorize(lcc_get_partial)(self.classification)\n",
    "    \n",
    "    def get_extent(self, normalized: bool = False) -> tuple[float, ...]:\n",
    "        if normalized:\n",
    "            if not hasattr(self, 'z_normalized'):\n",
    "                raise AttributeError(f'Attribute \\'z_normalized\\' does not exist')\n",
    "            return self.x.min(), self.x.max(), self.y.min(), self.y.max(), self.z_normalized.min(), self.z_normalized.max()\n",
    "        return self.x.min(), self.x.max(), self.y.min(), self.y.max(), self.z.min(), self.z.max()\n",
    "    \n",
    "\n",
    "    def apply_mask(self, mask: np.ndarray) -> None:\n",
    "        assert isinstance(mask, np.ndarray) and mask.dtype == bool, '\\'mask\\' must be a numpy.ndarray with dtype bool'\n",
    "        \n",
    "        for key, val in self.data.items():\n",
    "            if len(mask) == len(self.data[key]):\n",
    "                self.data[key] = val[mask]\n",
    "        self.point_count = len(self.data['x'])\n",
    "    \n",
    "    def add_dimension(self, dim: str, data: Any) -> None:\n",
    "        if dim in self.point_dimensions:\n",
    "            raise ValueError(f'Dimension {dim} already exists in las file')\n",
    "        self.point_dimensions.append(dim)\n",
    "        self.data[dim] = data\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        if name not in self.point_dimensions:\n",
    "            raise AttributeError(f'Attribute \\'{name}\\' does not exist')\n",
    "        return self.data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92842435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# las = LasPointData(las_files[1])\n",
    "las = LasPointData(las_file)\n",
    "las.point_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['classification', 'intensity', 'return_number', 'gps_time', 'x', 'y', 'z']\n",
    "# npdata = np.hstack([las.data[f].reshape(-1, 1) for f in features])\n",
    "# df = pd.DataFrame(npdata, columns=features)\n",
    "# df.classification = df.classification.map(lambda x: 1 if x == 6 else -1)\n",
    "\n",
    "# df[features[1:]].corrwith(df.classification.astype('float'), method=pointbiserialr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some info about the LiDAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are_points_compressed: False\n",
      "creation_date: 2023-02-25\n",
      "evlrs: []\n",
      "extra_header_bytes: b''\n",
      "extra_vlr_bytes: b''\n",
      "file_source_id: 0\n",
      "generating_software: ArcGIS\n",
      "global_encoding: <laspy.header.GlobalEncoding object at 0x31bf7d910>\n",
      "major_version: 1\n",
      "maxs: [4.83999999e+05 5.45800000e+06 7.48853000e+02]\n",
      "minor_version: 4\n",
      "mins: [4.8300e+05 5.4570e+06 1.4573e+01]\n",
      "number_of_evlrs: 0\n",
      "number_of_points_by_return: [40879944 25033837 15198490  7432236  2974999   980090   267791    61240\n",
      "    11732     1824      221       30        1        0        0]\n",
      "offset_to_point_data: 1003\n",
      "offsets: [     -0. 7000000.      -0.]\n",
      "point_count: 92842435\n",
      "point_format: <PointFormat(7, 0 bytes of extra dims)>\n",
      "scales: [0.001 0.001 0.001]\n",
      "start_of_first_evlr: 0\n",
      "start_of_waveform_data_packet_record: 0\n",
      "system_identifier: CONVERSION\n",
      "uuid: 00000000-0000-0000-0000-000000000000\n",
      "version: 1.4\n",
      "vlrs: [<VLR(user_id: 'ESRI_Export', record_id: '5', data len: 8)>, <VLR(user_id: 'ESRI_Export', record_id: '8', data len: 8)>, <laspy.vlrs.known.WktCoordinateSystemVlr object at 0x31c6c6990>]\n",
      "x_max: 483999.999\n",
      "x_min: 483000.0\n",
      "x_offset: -0.0\n",
      "x_scale: 0.001\n",
      "y_max: 5457999.999\n",
      "y_min: 5457000.0\n",
      "y_offset: 7000000.0\n",
      "y_scale: 0.001\n",
      "z_max: 748.8530000000001\n",
      "z_min: 14.573\n",
      "z_offset: -0.0\n",
      "z_scale: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Display header information\n",
    "las.header_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Projected CRS: EPSG:26910>\n",
      "Name: NAD83 / UTM zone 10N\n",
      "Axis Info [cartesian]:\n",
      "- E[east]: Easting (metre)\n",
      "- N[north]: Northing (metre)\n",
      "Area of Use:\n",
      "- undefined\n",
      "Coordinate Operation:\n",
      "- name: UTM zone 10N\n",
      "- method: Transverse Mercator\n",
      "Datum: North American Datum 1983\n",
      "- Ellipsoid: GRS 1980\n",
      "- Prime Meridian: Greenwich\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the CRS\n",
    "las.crs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PointFormat(7, 0 bytes of extra dims)>,\n",
       " ['x',\n",
       "  'y',\n",
       "  'z',\n",
       "  'X',\n",
       "  'Y',\n",
       "  'Z',\n",
       "  'intensity',\n",
       "  'return_number',\n",
       "  'number_of_returns',\n",
       "  'synthetic',\n",
       "  'key_point',\n",
       "  'withheld',\n",
       "  'overlap',\n",
       "  'scanner_channel',\n",
       "  'scan_direction_flag',\n",
       "  'edge_of_flight_line',\n",
       "  'classification',\n",
       "  'user_data',\n",
       "  'scan_angle',\n",
       "  'point_source_id',\n",
       "  'gps_time',\n",
       "  'red',\n",
       "  'green',\n",
       "  'blue'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the point format dimensions\n",
    "las.get_point_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply some masks to remove unwanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell naively downsamples the data by selecting only every kth point\n",
    "# # This was used for testing below algos with lower computational cost before running the whole dataset\n",
    "\n",
    "# # Create a mask to select only every kth point\n",
    "# k = 20\n",
    "# mask = np.zeros(las.point_count, dtype=bool)\n",
    "# mask[::k] = True\n",
    "\n",
    "# las.apply_mask(mask)\n",
    "# print(f'{np.sum(~mask)}/{len(mask)} points removed')\n",
    "# del mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23154/92842435 points removed\n"
     ]
    }
   ],
   "source": [
    "# This cell removes points that are labelled as noise in the las file.\n",
    "\n",
    "# Remove labeled noise\n",
    "# 7 is the LIDAR classification code for noise\n",
    "noise_mask = np.ones(las.point_count, dtype=bool)\n",
    "noise_mask[las.classification == 7] = False\n",
    "\n",
    "las.apply_mask(noise_mask)\n",
    "print(f'{np.sum(~noise_mask)}/{len(noise_mask)} points removed')\n",
    "del noise_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6302188/92819281 points removed\n"
     ]
    }
   ],
   "source": [
    "# This cell removes points that are labelled as never classified or unclassified in the las file.\n",
    "\n",
    "# Remove labeled noise\n",
    "# 0 is the LIDAR classification code for never assigned\n",
    "# 1 is the LIDAR classification code for unassigned\n",
    "unclassified_mask = np.ones(las.point_count, dtype=bool)\n",
    "unclassified_mask[(las.classification == 0) | (las.classification == 1)] = False\n",
    "\n",
    "las.apply_mask(unclassified_mask)\n",
    "print(f'{np.sum(~unclassified_mask)}/{len(unclassified_mask)} points removed')\n",
    "del unclassified_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is an alternative noise removal method that removes noise though statistical outlier rejection\n",
    "\n",
    "# # Use statistical outlier removal to remove noise\n",
    "# point_data = las.get_point_data()\n",
    "# kdtree = KDTree(point_data)  # Use a KDTree for fast nearest neighbour search\n",
    "\n",
    "# k = 20  # Number of neighbors\n",
    "# z_thresh = 15.0  # Z-score threshold for outliers\n",
    "\n",
    "# # Compute distances to k nearest neighbors\n",
    "# dists, _ = kdtree.query(point_data, k=k + 1)  # k + 1 because query includes the point itself\n",
    "# z_scores = np.abs((dists.mean(axis=1) - dists.mean()) / dists.std())\n",
    "\n",
    "# del kdtree, dists\n",
    "\n",
    "# # Filter points\n",
    "# sor_mask = z_scores < z_thresh\n",
    "# las.apply_mask(sor_mask)\n",
    "\n",
    "# print(f'{np.sum(~sor_mask)}/{len(sor_mask)} points removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66026771/86517093 points removed\n"
     ]
    }
   ],
   "source": [
    "# Crop the region to 1/4 of its original area\n",
    "\n",
    "point_data = las.get_point_data()\n",
    "\n",
    "xmin = min(point_data[:, 0])\n",
    "ymin = min(point_data[:, 1])\n",
    "xmax = max(point_data[:, 0])\n",
    "ymax = max(point_data[:, 1])\n",
    "\n",
    "xrange = max(point_data[:, 0]) - xmin\n",
    "yrange = max(point_data[:, 1]) - ymin\n",
    "\n",
    "if 'kits' in str(data_folder):\n",
    "    xmax = xmin + xrange / 2.0\n",
    "    ymax = ymin + yrange / 2.0\n",
    "else:\n",
    "    xmin = xmax - xrange / 2.0\n",
    "    ymin = ymax - xrange / 2.0\n",
    "\n",
    "range_mask = np.zeros(las.point_count, dtype=bool)\n",
    "range_mask[(point_data[:, 0] <= xmax) & (point_data[:, 0] >= xmin) & (point_data[:, 1] < ymax) & (point_data[:, 1] >= ymin)] = True\n",
    "\n",
    "las.apply_mask(range_mask)\n",
    "print(f'{np.sum(~range_mask)}/{len(range_mask)} points removed')\n",
    "del range_mask, point_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify unclassified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 unclassified points classified using nearest neighbour interpolation\n"
     ]
    }
   ],
   "source": [
    "# This cell reclassifies all unclassified points using nearest neighbour classification\n",
    "# 1 is the LIDAR classification code for unclassified points\n",
    "\n",
    "# Reclassify unclassified points using nearest neighbour classification\n",
    "point_data = las.get_point_data()\n",
    "\n",
    "# Separate classified and unclassified points\n",
    "classified_mask = las.classification != 1\n",
    "classified_points = point_data[classified_mask]\n",
    "classified_classes = las.classification[classified_mask]\n",
    "unclassified_points = point_data[~classified_mask]\n",
    "\n",
    "# Use a KDTree to get the indices of the nearest classified points to each unclassified point\n",
    "kdtree = KDTree(classified_points)\n",
    "_, neighbour_indices = kdtree.query(unclassified_points, k=20)\n",
    "\n",
    "# Get the most common class of the neighbours of each unclassified point\n",
    "neighbour_classes = classified_classes[neighbour_indices]\n",
    "most_common_neighbour_classes = scipy_mode(neighbour_classes, axis=1).mode\n",
    "\n",
    "# Update the classes of the unclassified points\n",
    "las.classification[~classified_mask] = most_common_neighbour_classes\n",
    "print(f'{sum(~classified_mask)} unclassified points classified using nearest neighbour interpolation')\n",
    "\n",
    "# Delete large vairables to conserve memory\n",
    "del classified_mask, kdtree, classified_points, classified_classes, unclassified_points\n",
    "del point_data, neighbour_indices, neighbour_classes, most_common_neighbour_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the height above ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell uses a CSF to separate ground points from non-gound points\n",
    "\n",
    "# # Use a cloth simulation filter to separate ground points from non-ground points\n",
    "# point_data = las.get_point_data()\n",
    "\n",
    "# csf = CSF.CSF()\n",
    "\n",
    "# csf.params.bSloopSmooth = False\n",
    "# # csf.params.cloth_resolution = 0.5\n",
    "# # csf.params.rigidness = 3\n",
    "\n",
    "# # These can be left as default\n",
    "# # csf.params.time_step = 0.65\n",
    "# # csf.params.class_threshold = 0.5\n",
    "# # csf.params.interations = 500\n",
    "\n",
    "# csf.setPointCloud(point_data)\n",
    "\n",
    "# ground_indices = CSF.VecInt()  # a list to indicate the index of ground points after calculation\n",
    "# non_ground_indices = CSF.VecInt() # a list to indicate the index of non-ground points after calculation\n",
    "# csf.do_filtering(ground_indices, non_ground_indices) # do actual filtering.\n",
    "# ground_indices = np.asarray(ground_indices)\n",
    "# non_ground_indices = np.asarray(non_ground_indices)\n",
    "# ground = point_data[ground_indices]\n",
    "# non_ground = point_data[non_ground_indices]\n",
    "\n",
    "# os.remove('cloth_nodes.txt')\n",
    "# del csf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses the provided classification to separate ground and non-ground points\n",
    "\n",
    "point_data = las.get_point_data()\n",
    "las_classification = las.classification\n",
    "ground_indices = np.arange(len(las_classification))[las_classification == 2]\n",
    "non_ground_indices = np.arange(len(las_classification))[las_classification != 2]\n",
    "ground = point_data[ground_indices]\n",
    "non_ground = point_data[non_ground_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the ground points to get the ground elevation of the non-ground points\n",
    "\n",
    "griddata_partial = analysis_utils.GriddataPartial(ground=ground, method='linear')\n",
    "non_ground_xy_split = np.array_split(non_ground[:, :2], PROCESSES)\n",
    "with Pool(PROCESSES) as pool:\n",
    "    non_ground_z_proj_split = pool.map(griddata_partial, non_ground_xy_split)\n",
    "non_ground_z_proj = np.hstack(non_ground_z_proj_split) # ground elevation of each non-ground point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some values of non_ground_z_proj are nan, so we use nearest neighbour interpolation to fill in these values\n",
    "# This is much faster than the interpolation above since we are using a nearest neighbour interpolation instead of a linear one\n",
    "griddata_partial = analysis_utils.GriddataPartial(ground=ground, method='nearest')\n",
    "\n",
    "nan_mask = np.isnan(non_ground_z_proj)\n",
    "non_ground_xy_giving_nan = non_ground[nan_mask, :2]  # xy points giving nan values with the linear interpolation \n",
    "\n",
    "non_ground_z_proj_nan_fixed = griddata_partial(non_ground_xy_giving_nan)  # re-interpolate using a nn approach\n",
    "non_ground_z_proj[nan_mask] = non_ground_z_proj_nan_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the height above ground of all points\n",
    "non_ground_heights = non_ground[:, 2] - non_ground_z_proj  # height above ground of each non-ground point\n",
    "\n",
    "point_data_normalized_height = point_data.copy()\n",
    "point_data_normalized_height[ground_indices, 2] = 0\n",
    "point_data_normalized_height[non_ground_indices, 2] = non_ground_heights\n",
    "\n",
    "# Delete large variables to conserve memory\n",
    "del ground_indices, non_ground_indices, ground, non_ground, point_data, griddata_partial\n",
    "del non_ground_xy_split, non_ground_z_proj_split, non_ground_z_proj, non_ground_heights\n",
    "del nan_mask, non_ground_xy_giving_nan, non_ground_z_proj_nan_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.isnan(point_data_normalized_height).any(), 'Some normalized height vals are nan!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh_path = data_folder / 'normalized_height_linear.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalized height data to avoid recomputing it later\n",
    "with open(nh_path, 'wb') as f:\n",
    "    dill.dump(point_data_normalized_height, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalized height data\n",
    "with open(nh_path, 'rb') as f:\n",
    "    point_data_normalized_height = dill.load(f)\n",
    "las.add_dimension('z_normalized', point_data_normalized_height[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the height variation\n",
    "\n",
    "Height variation is the absolute difference between min and max values of normalized height within a disk of radius $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a kd-tree to compute the height variation of all points\n",
    "\n",
    "point_data_normalized_height_xy = point_data_normalized_height[:, :2]\n",
    "kdtree = KDTree(point_data_normalized_height_xy)\n",
    "\n",
    "# Compute height variation by querying the kd-tree for each point in the point cloud\n",
    "# Use multiple processes to speed up computation\n",
    "height_variation_partial = analysis_utils.HeightVariation(point_data_normalized_height, kdtree, r, method='mad')\n",
    "\n",
    "n = len(point_data_normalized_height)\n",
    "if PROCESSES == 1:\n",
    "    index_endpoints = [(0, n)]\n",
    "else:\n",
    "    chunk_size = n // PROCESSES\n",
    "    index_endpoints = [(chunk_size * i, chunk_size * (i + 1)) for i in range(PROCESSES - 1)]\n",
    "    index_endpoints.append((index_endpoints[-1][1], n))\n",
    "\n",
    "with Pool(PROCESSES) as pool:\n",
    "    height_variation_split = pool.map(height_variation_partial, index_endpoints)\n",
    "height_variation = np.hstack(height_variation_split)\n",
    "\n",
    "del kdtree, point_data_normalized_height_xy, point_data_normalized_height, height_variation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_path = data_folder / f'height_variation_{\"1m\" if r == 1.0 else f\"{int(r * 100)}cm\"}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the height_variation data to avoid recomputing it later\n",
    "with open(hv_path, 'wb') as f:\n",
    "    dill.dump(height_variation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the height variation\n",
    "with open(hv_path, 'rb') as f:\n",
    "    height_variation = dill.load(f)\n",
    "las.add_dimension('height_variation', height_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the normal variation\n",
    "\n",
    "Normal variation is the negative of the average dot product of each normal with other normals within a disk of radius $r$. This value gives a measure of planarity near each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use open3d to estimate surface normals\n",
    "\n",
    "point_data = las.get_point_data(normalized=True)\n",
    "geom = o3d.geometry.PointCloud()\n",
    "geom.points = o3d.utility.Vector3dVector(point_data)\n",
    "\n",
    "k = 100\n",
    "geom.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.5, max_nn=k))\n",
    "direction = np.array([1.0, 0.0, 1.0]) if 'downtown' in str(data_folder) else np.ones(3, dtype=float)\n",
    "geom.orient_normals_to_align_with_direction(direction)\n",
    "# geom.orient_normals_consistent_tangent_plane(k=k)  # RuntimeError\n",
    "normals = np.asarray(geom.normals)\n",
    "\n",
    "del geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20490322/20490322 [05:26<00:00, 62792.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute the normal variation\n",
    "\n",
    "point_data_xy = point_data[:, :2]\n",
    "kdtree = KDTree(point_data_xy)\n",
    "\n",
    "normal_variation_partial = analysis_utils.NormalVariation(point_data_xy, normals, kdtree, r, use_tqdm=True)\n",
    "n = len(point_data_xy)\n",
    "\n",
    "# if PROCESSES == 1:\n",
    "#     index_endpoints = [(0, n)]\n",
    "# else:\n",
    "#     chunk_size = n // PROCESSES\n",
    "#     index_endpoints = [(chunk_size * i, chunk_size * (i + 1)) for i in range(PROCESSES - 1)]\n",
    "#     index_endpoints.append((index_endpoints[-1][1], n))\n",
    "\n",
    "# with Pool(PROCESSES) as pool:\n",
    "#     normal_variation_split = pool.map(normal_variation_partial, index_endpoints)\n",
    "# normal_variation = np.hstack(normal_variation_split)\n",
    "\n",
    "normal_variation = normal_variation_partial((0, n))\n",
    "\n",
    "del kdtree, point_data_xy, point_data, normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_path = data_folder / f'normal_variation_{\"1m\" if r == 1.0 else f\"{int(r * 100)}cm\"}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the normal variation data to avoid recomputing it later\n",
    "with open(nv_path, 'wb') as f:\n",
    "    dill.dump(normal_variation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normal variation\n",
    "with open(nv_path, 'rb') as f:\n",
    "    normal_variation = dill.load(f)\n",
    "las.add_dimension('normal_variation', normal_variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample the point cloud and associated data using voxel downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20490322/20490322 [00:49<00:00, 418012.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with each entry corresponding to one voxel\n",
    "\n",
    "points = las.get_point_data(normalized=True)\n",
    "voxel_size = 1.0  # One point per cubic meter\n",
    "voxel_indices = np.floor(points / voxel_size).astype(int)\n",
    "\n",
    "# Use a dictionary to store the sum of points and counts for each voxel\n",
    "voxel_dict = {}\n",
    "for voxel, point, hv, nv, its, cls in tqdm(zip(voxel_indices, points, height_variation, normal_variation, las.intensity.astype(np.uint64), las.classification), total=len(points)):\n",
    "    voxel_key = tuple(voxel)\n",
    "    if voxel_key in voxel_dict:\n",
    "        voxel_dict[voxel_key][0] += point\n",
    "        voxel_dict[voxel_key][1] += hv\n",
    "        voxel_dict[voxel_key][2] += nv\n",
    "        voxel_dict[voxel_key][3] += its\n",
    "        voxel_dict[voxel_key][4].append(cls)\n",
    "        voxel_dict[voxel_key][5] += 1\n",
    "    else:\n",
    "        voxel_dict[voxel_key] = [point, hv, nv, its, [cls], 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2176814/2176814 [00:10<00:00, 207070.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute the average for each voxel and create the downsampled point cloud\n",
    "downsampled_points = np.empty((len(voxel_dict), 7))\n",
    "for i, ((x, y, z), hv, nv, its, cls, k) in tqdm(enumerate(voxel_dict.values()), total=len(voxel_dict)):\n",
    "    downsampled_points[i] = np.array([x / k, y / k, z / k, hv / k, nv / k, its / k, stats_mode(cls)])\n",
    "\n",
    "del voxel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe containing the data of interest from each point in the downsampled point cloud and save it as a csv\n",
    "\n",
    "columns = ['x', 'y', 'z', 'height_variation', 'normal_variation', 'intensity', 'classification']\n",
    "df = pd.DataFrame(downsampled_points, columns=columns)\n",
    "\n",
    "df['classification'] = df['classification'].astype(int)\n",
    "df[['x', 'y', 'z', 'intensity']] = df[['x', 'y', 'z', 'intensity']].round(2)\n",
    "df[['height_variation', 'normal_variation']] = df[['height_variation', 'normal_variation']].round(4)\n",
    "\n",
    "df.to_csv(data_folder / f'{\"1m\" if voxel_size == 1.0 else f\"{int(voxel_size*100)}cm\"}_lidar.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>height_variation</th>\n",
       "      <th>normal_variation</th>\n",
       "      <th>intensity</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483500.53</td>\n",
       "      <td>5457500.47</td>\n",
       "      <td>8.58</td>\n",
       "      <td>2.9096</td>\n",
       "      <td>-0.4244</td>\n",
       "      <td>46.62</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483537.32</td>\n",
       "      <td>5457500.31</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>-0.3056</td>\n",
       "      <td>66.75</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>483567.37</td>\n",
       "      <td>5457500.48</td>\n",
       "      <td>32.16</td>\n",
       "      <td>3.1157</td>\n",
       "      <td>-0.7190</td>\n",
       "      <td>341.96</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>483596.23</td>\n",
       "      <td>5457500.09</td>\n",
       "      <td>43.17</td>\n",
       "      <td>2.3543</td>\n",
       "      <td>-0.0939</td>\n",
       "      <td>136.60</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>483585.47</td>\n",
       "      <td>5457500.00</td>\n",
       "      <td>39.73</td>\n",
       "      <td>3.6597</td>\n",
       "      <td>-0.6882</td>\n",
       "      <td>164.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x           y      z  height_variation  normal_variation  \\\n",
       "0  483500.53  5457500.47   8.58            2.9096           -0.4244   \n",
       "1  483537.32  5457500.31   6.44            0.9901           -0.3056   \n",
       "2  483567.37  5457500.48  32.16            3.1157           -0.7190   \n",
       "3  483596.23  5457500.09  43.17            2.3543           -0.0939   \n",
       "4  483585.47  5457500.00  39.73            3.6597           -0.6882   \n",
       "\n",
       "   intensity  classification  \n",
       "0      46.62               5  \n",
       "1      66.75               5  \n",
       "2     341.96               5  \n",
       "3     136.60               5  \n",
       "4     164.00               5  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_folder / '1m_lidar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "# points = las.get_point_data()\n",
    "# points[:, 2] = 0\n",
    "# points = downsampled_points[:, :3]  \n",
    "points = df[['x','y','z']].to_numpy()  # The downsampled, processed point cloud\n",
    "\n",
    "geom = o3d.geometry.PointCloud()\n",
    "geom.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "# Use a colormap from Matplotlib\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Map integers to colors using the colormap\n",
    "# colour_var = np.asarray(las.height_variation)\n",
    "colour_var = np.asarray(df.height_variation)\n",
    "colour_var_norm = (colour_var - colour_var.min()) / (colour_var.max() - colour_var.min())\n",
    "colors = cmap(colour_var_norm)[:, :3]  # [:3] to exclude the alpha channel\n",
    "geom.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "o3d.visualization.draw_geometries([geom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
